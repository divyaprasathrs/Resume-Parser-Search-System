{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Divya_prasath\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\IPython\\core\\interactiveshell.py:3577: LangChainDeprecationWarning: As of langchain-core 0.3.0, LangChain uses pydantic v2 internally. The langchain_core.pydantic_v1 module was a compatibility shim for pydantic v1, and should no longer be used. Please update the code to import from Pydantic directly.\n",
      "\n",
      "For example, replace imports like: `from langchain_core.pydantic_v1 import BaseModel`\n",
      "with: `from pydantic import BaseModel`\n",
      "or the v1 compatibility namespace if you are working in a code base that has not been fully upgraded to pydantic 2 yet. \tfrom pydantic.v1 import BaseModel\n",
      "\n",
      "  exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "c:\\Users\\Divya_prasath\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\elasticsearch\\_sync\\client\\__init__.py:403: SecurityWarning: Connecting to 'https://localhost:9200' using TLS with verify_certs=False is insecure\n",
      "  _transport = transport_class(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inserted record into Elasticsearch\n",
      "Inserted record into Elasticsearch\n",
      "Inserted record into Elasticsearch\n",
      "Inserted record into Elasticsearch\n",
      "Inserted record into Elasticsearch\n",
      "Inserted record into Elasticsearch\n",
      "Inserted record into Elasticsearch\n",
      "Inserted record into Elasticsearch\n",
      "Inserted record into Elasticsearch\n",
      "Inserted record into Elasticsearch\n",
      "Inserted record into Elasticsearch\n",
      "Inserted record into Elasticsearch\n",
      "Inserted record into Elasticsearch\n",
      "Inserted record into Elasticsearch\n",
      "Inserted record into Elasticsearch\n",
      "Inserted record into Elasticsearch\n",
      "Inserted record into Elasticsearch\n",
      "Inserted record into Elasticsearch\n",
      "Inserted record into Elasticsearch\n",
      "Inserted record into Elasticsearch\n",
      "Inserted record into Elasticsearch\n",
      "Inserted record into Elasticsearch\n",
      "Inserted record into Elasticsearch\n",
      "Inserted record into Elasticsearch\n",
      "Inserted record into Elasticsearch\n",
      "Inserted record into Elasticsearch\n",
      "Inserted record into Elasticsearch\n",
      "Inserted record into Elasticsearch\n",
      "Inserted record into Elasticsearch\n",
      "Inserted record into Elasticsearch\n",
      "Inserted record into Elasticsearch\n",
      "Inserted record into Elasticsearch\n",
      "Inserted record into Elasticsearch\n",
      "Inserted record into Elasticsearch\n",
      "Inserted record into Elasticsearch\n",
      "Inserted record into Elasticsearch\n",
      "Inserted record into Elasticsearch\n",
      "Inserted record into Elasticsearch\n",
      "Inserted record into Elasticsearch\n",
      "Inserted record into Elasticsearch\n",
      "Inserted record into Elasticsearch\n",
      "Inserted record into Elasticsearch\n",
      "Inserted record into Elasticsearch\n",
      "Inserted record into Elasticsearch\n",
      "Inserted record into Elasticsearch\n",
      "Inserted record into Elasticsearch\n",
      "Inserted record into Elasticsearch\n",
      "Inserted record into Elasticsearch\n",
      "Inserted record into Elasticsearch\n",
      "Inserted record into Elasticsearch\n",
      "Inserted record into Elasticsearch\n",
      "Inserted record into Elasticsearch\n",
      "Inserted record into Elasticsearch\n",
      "Inserted record into Elasticsearch\n",
      "Inserted record into Elasticsearch\n",
      "Inserted record into Elasticsearch\n",
      "Inserted record into Elasticsearch\n",
      "Inserted record into Elasticsearch\n",
      "Inserted record into Elasticsearch\n",
      "Inserted record into Elasticsearch\n",
      "Inserted record into Elasticsearch\n",
      "Inserted record into Elasticsearch\n",
      "Inserted record into Elasticsearch\n",
      "Inserted record into Elasticsearch\n",
      "Inserted record into Elasticsearch\n",
      "Inserted record into Elasticsearch\n",
      "Inserted record into Elasticsearch\n",
      "Inserted record into Elasticsearch\n",
      "Inserted record into Elasticsearch\n",
      "Inserted record into Elasticsearch\n",
      "Inserted record into Elasticsearch\n",
      "Inserted record into Elasticsearch\n",
      "Inserted record into Elasticsearch\n",
      "Inserted record into Elasticsearch\n",
      "Inserted record into Elasticsearch\n",
      "Inserted record into Elasticsearch\n",
      "Inserted record into Elasticsearch\n",
      "Inserted record into Elasticsearch\n",
      "Inserted record into Elasticsearch\n",
      "Inserted record into Elasticsearch\n",
      "Inserted record into Elasticsearch\n",
      "Inserted record into Elasticsearch\n",
      "Inserted record into Elasticsearch\n",
      "Inserted record into Elasticsearch\n",
      "Inserted record into Elasticsearch\n",
      "Inserted record into Elasticsearch\n",
      "Inserted record into Elasticsearch\n",
      "Inserted record into Elasticsearch\n",
      "Inserted record into Elasticsearch\n",
      "Inserted record into Elasticsearch\n",
      "Inserted record into Elasticsearch\n",
      "Inserted record into Elasticsearch\n",
      "Inserted record into Elasticsearch\n",
      "Inserted record into Elasticsearch\n",
      "Inserted record into Elasticsearch\n",
      "Inserted record into Elasticsearch\n",
      "Inserted record into Elasticsearch\n",
      "Inserted record into Elasticsearch\n",
      "Inserted record into Elasticsearch\n",
      "Inserted record into Elasticsearch\n",
      "Inserted record into Elasticsearch\n",
      "Inserted record into Elasticsearch\n",
      "Inserted record into Elasticsearch\n",
      "Inserted record into Elasticsearch\n",
      "Inserted record into Elasticsearch\n",
      "Inserted DhineshKumar_SDE_Resume.pdf into Elasticsearch\n",
      "Inserted hi.pdf into Elasticsearch\n",
      "Inserted Jaya Sanjay - Resume.pdf into Elasticsearch\n",
      "Inserted NILESH SRINIVASAN.pdf into Elasticsearch\n",
      "Data extraction and indexing completed successfully!\n"
     ]
    }
   ],
   "source": [
    "from elasticsearch import Elasticsearch, helpers\n",
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "import time\n",
    "import pdfplumber\n",
    "import xml.etree.ElementTree as ET\n",
    "from docx import Document\n",
    "from win32com import client\n",
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "import urllib3\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "\n",
    "load_dotenv()\n",
    "urllib3.disable_warnings()\n",
    "\n",
    "\n",
    "ES_CONFIG = {\n",
    "    \"host\": \"https://localhost:9200\",\n",
    "    \"username\": \"elastic\",\n",
    "    \"password\": os.getenv(\"ES_PASSWORD\"),  \n",
    "    \"index\": \"task5\"\n",
    "}\n",
    "\n",
    "es = Elasticsearch(\n",
    "    ES_CONFIG[\"host\"],\n",
    "    basic_auth=(ES_CONFIG[\"username\"], ES_CONFIG[\"password\"]),\n",
    "    verify_certs=False\n",
    ")\n",
    "\n",
    "\n",
    "csv_file_path = r\"C:\\Users\\Divya_prasath\\Desktop\\task\\extracted_data\\resume.csv\"\n",
    "\n",
    "def push_data_to_elasticsearch(csv_file, es_client, index_name):\n",
    "    if not os.path.exists(csv_file):\n",
    "        print(\"CSV file not found!\")\n",
    "        return\n",
    "    \n",
    "    df = pd.read_csv(csv_file)\n",
    "    df.replace({np.nan: None}, inplace=True)\n",
    "    records = df.to_dict(orient=\"records\")\n",
    "\n",
    "    for record in records:\n",
    "        try:\n",
    "            es_client.create(index=index_name, id=str(time.time()), document=record)\n",
    "            print(f\"Inserted record into Elasticsearch\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error inserting record: {e}\")\n",
    "\n",
    "push_data_to_elasticsearch(csv_file_path, es, ES_CONFIG[\"index\"])\n",
    "\n",
    "\n",
    "\n",
    "def extract_text_from_pdf(pdf_path):\n",
    "    with pdfplumber.open(pdf_path) as pdf:\n",
    "        return \"\\n\".join([page.extract_text() for page in pdf.pages if page.extract_text()])\n",
    "\n",
    "def extract_text_from_docx(docx_path):\n",
    "    doc = Document(docx_path)\n",
    "    return \"\\n\".join([para.text for para in doc.paragraphs if para.text.strip()])\n",
    "\n",
    "def extract_text_from_doc(doc_path):\n",
    "    try:\n",
    "        word = client.Dispatch(\"Word.Application\")\n",
    "        word.Visible = False\n",
    "        doc = word.Documents.Open(doc_path)\n",
    "        text = doc.Content.Text.strip()\n",
    "        doc.Close(False)\n",
    "        word.Quit()\n",
    "        return text\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing DOC file {doc_path}: {e}\")\n",
    "        return \"\"\n",
    "\n",
    "def extract_text_from_xml(xml_path):\n",
    "    try:\n",
    "        tree = ET.parse(xml_path)\n",
    "        root = tree.getroot()\n",
    "        return \" \".join([elem.text.strip() for elem in root.iter() if elem.text])\n",
    "    except Exception as e:\n",
    "        print(f\"Error parsing XML {xml_path}: {e}\")\n",
    "        return \"\"\n",
    "\n",
    "def extract_text_from_folder(folder_path):\n",
    "    extracted_texts = {}\n",
    "    for filename in os.listdir(folder_path):\n",
    "        file_path = os.path.join(folder_path, filename)\n",
    "        text = None\n",
    "\n",
    "        try:\n",
    "            if filename.lower().endswith(\".pdf\"):\n",
    "                text = extract_text_from_pdf(file_path)\n",
    "            elif filename.lower().endswith(\".docx\"):\n",
    "                text = extract_text_from_docx(file_path)\n",
    "            elif filename.lower().endswith(\".doc\"):\n",
    "                text = extract_text_from_doc(file_path)\n",
    "            elif filename.lower().endswith(\".xml\"):\n",
    "                text = extract_text_from_xml(file_path)\n",
    "\n",
    "            if text and text.strip():\n",
    "                extracted_texts[filename] = text\n",
    "            else:\n",
    "                print(f\"Skipping empty file: {filename}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error extracting text from {filename}: {e}\")\n",
    "\n",
    "    return extracted_texts\n",
    "\n",
    "folder_path = r\"C:\\Users\\Divya_prasath\\Desktop\\task\\index resumes\"\n",
    "text_files = extract_text_from_folder(folder_path)\n",
    "\n",
    "\n",
    "api_key = os.getenv(\"GOOGLE_API_KEY\")  \n",
    "if not api_key:\n",
    "    raise ValueError(\"GEMINI_API_KEY is not set. Please set it as an environment variable.\")\n",
    "\n",
    "llm = ChatGoogleGenerativeAI(\n",
    "    model=\"gemini-1.5-flash\",\n",
    "    temperature=0,\n",
    "    max_tokens=None,\n",
    "    timeout=None,\n",
    "    max_retries=2,\n",
    "    api_key=api_key,\n",
    ")\n",
    "\n",
    "\n",
    "class Resume(BaseModel):\n",
    "    name: str = Field(description=\"name from resume\")\n",
    "    phone: str = Field(description=\"phone number from resume\")\n",
    "    email: str = Field(description=\"email from resume\")\n",
    "    skill: str = Field(description=\"skill from resume\")\n",
    "\n",
    "parser = JsonOutputParser(pydantic_object=Resume)\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    template=\"Extract name, phone, email, skills from the given text resume.\\n{format_instructions}\\n{query}\\n\",\n",
    "    input_variables=[\"query\"],\n",
    "    partial_variables={\"format_instructions\": parser.get_format_instructions()},\n",
    ")\n",
    "\n",
    "chain = prompt | llm | parser\n",
    "\n",
    "for filename, text in text_files.items():\n",
    "    retries = 3\n",
    "    success = False\n",
    "\n",
    "    while retries > 0 and not success:\n",
    "        try:\n",
    "            extracted_data = chain.invoke({\"query\": text})\n",
    "            extracted_details = Resume(**extracted_data) \n",
    "            doc = extracted_details.dict()  \n",
    "\n",
    "            \n",
    "            try:\n",
    "                es.create(index=ES_CONFIG[\"index\"], id=str(time.time()), document=doc)\n",
    "                print(f\"Inserted {filename} into Elasticsearch\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error inserting {filename}: {e}\")\n",
    "\n",
    "            success = True\n",
    "            time.sleep(1)\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {filename}: {e}\")\n",
    "            retries -= 1\n",
    "            time.sleep(2 ** (3 - retries))  \n",
    "\n",
    "print(\"Data extraction and indexing completed successfully!\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

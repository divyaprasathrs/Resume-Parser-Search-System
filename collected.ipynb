{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error extracting text from Naukri_JAYAKUMART[2y_5m].doc: (-2147221005, 'Invalid class string', None, None)\n",
      "Error extracting text from Naukri_MrRamesh[12y_0m].doc: (-2147221005, 'Invalid class string', None, None)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrying langchain_google_genai.chat_models._chat_with_retry.<locals>._chat_with_retry in 2.0 seconds as it raised ResourceExhausted: 429 Resource has been exhausted (e.g. check quota)..\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing Naukri_DuvarakeshRavi[3y_3m] 1.pdf: 429 Resource has been exhausted (e.g. check quota).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrying langchain_google_genai.chat_models._chat_with_retry.<locals>._chat_with_retry in 2.0 seconds as it raised ResourceExhausted: 429 Resource has been exhausted (e.g. check quota)..\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing Naukri_DuvarakeshRavi[3y_3m] 1.pdf: 429 Resource has been exhausted (e.g. check quota).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrying langchain_google_genai.chat_models._chat_with_retry.<locals>._chat_with_retry in 2.0 seconds as it raised ResourceExhausted: 429 Resource has been exhausted (e.g. check quota)..\n",
      "Retrying langchain_google_genai.chat_models._chat_with_retry.<locals>._chat_with_retry in 2.0 seconds as it raised ResourceExhausted: 429 Resource has been exhausted (e.g. check quota)..\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing Naukri_JisinJoseph[1y_9m].pdf: 429 Resource has been exhausted (e.g. check quota).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrying langchain_google_genai.chat_models._chat_with_retry.<locals>._chat_with_retry in 2.0 seconds as it raised ResourceExhausted: 429 Resource has been exhausted (e.g. check quota)..\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing Naukri_JisinJoseph[1y_9m].pdf: 429 Resource has been exhausted (e.g. check quota).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrying langchain_google_genai.chat_models._chat_with_retry.<locals>._chat_with_retry in 2.0 seconds as it raised ResourceExhausted: 429 Resource has been exhausted (e.g. check quota)..\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing Naukri_JisinJoseph[1y_9m].pdf: 429 Resource has been exhausted (e.g. check quota).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrying langchain_google_genai.chat_models._chat_with_retry.<locals>._chat_with_retry in 2.0 seconds as it raised ResourceExhausted: 429 Resource has been exhausted (e.g. check quota)..\n",
      "Retrying langchain_google_genai.chat_models._chat_with_retry.<locals>._chat_with_retry in 2.0 seconds as it raised ResourceExhausted: 429 Resource has been exhausted (e.g. check quota)..\n",
      "Retrying langchain_google_genai.chat_models._chat_with_retry.<locals>._chat_with_retry in 2.0 seconds as it raised ResourceExhausted: 429 Resource has been exhausted (e.g. check quota)..\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing Naukri_RabinsinghR[3y_5m].pdf: 429 Resource has been exhausted (e.g. check quota).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrying langchain_google_genai.chat_models._chat_with_retry.<locals>._chat_with_retry in 2.0 seconds as it raised ResourceExhausted: 429 Resource has been exhausted (e.g. check quota)..\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing Naukri_RabinsinghR[3y_5m].pdf: 429 Resource has been exhausted (e.g. check quota).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrying langchain_google_genai.chat_models._chat_with_retry.<locals>._chat_with_retry in 2.0 seconds as it raised ResourceExhausted: 429 Resource has been exhausted (e.g. check quota)..\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing Naukri_RabinsinghR[3y_5m].pdf: 429 Resource has been exhausted (e.g. check quota).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrying langchain_google_genai.chat_models._chat_with_retry.<locals>._chat_with_retry in 2.0 seconds as it raised ResourceExhausted: 429 Resource has been exhausted (e.g. check quota)..\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing Naukri_SheikAbdulla[1y_3m] 1.docx: 429 Resource has been exhausted (e.g. check quota).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrying langchain_google_genai.chat_models._chat_with_retry.<locals>._chat_with_retry in 2.0 seconds as it raised ResourceExhausted: 429 Resource has been exhausted (e.g. check quota)..\n",
      "Retrying langchain_google_genai.chat_models._chat_with_retry.<locals>._chat_with_retry in 2.0 seconds as it raised ResourceExhausted: 429 Resource has been exhausted (e.g. check quota)..\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing Naukri_VetrivelMani[9y_0m].pdf: 429 Resource has been exhausted (e.g. check quota).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrying langchain_google_genai.chat_models._chat_with_retry.<locals>._chat_with_retry in 2.0 seconds as it raised ResourceExhausted: 429 Resource has been exhausted (e.g. check quota)..\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing Naukri_VetrivelMani[9y_0m].pdf: 429 Resource has been exhausted (e.g. check quota).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrying langchain_google_genai.chat_models._chat_with_retry.<locals>._chat_with_retry in 2.0 seconds as it raised ResourceExhausted: 429 Resource has been exhausted (e.g. check quota)..\n",
      "Retrying langchain_google_genai.chat_models._chat_with_retry.<locals>._chat_with_retry in 2.0 seconds as it raised ResourceExhausted: 429 Resource has been exhausted (e.g. check quota)..\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing Naukri_VigneshMohandas[5y_3m].pdf: 429 Resource has been exhausted (e.g. check quota).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrying langchain_google_genai.chat_models._chat_with_retry.<locals>._chat_with_retry in 2.0 seconds as it raised ResourceExhausted: 429 Resource has been exhausted (e.g. check quota)..\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted resume details saved to: C:\\Users\\Divya_prasath\\Desktop\\task\\extracted_data\\resume.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pdfplumber\n",
    "import pandas as pd\n",
    "import time\n",
    "import xml.etree.ElementTree as ET\n",
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from docx import Document \n",
    "from win32com import client \n",
    "\n",
    "# Function to extract text from different file formats\n",
    "def extract_text_from_pdf(pdf_path):\n",
    "    with pdfplumber.open(pdf_path) as pdf:\n",
    "        return \"\".join([page.extract_text() + \"\\n\" for page in pdf.pages])\n",
    "\n",
    "def extract_text_from_docx(docx_path):\n",
    "    doc = Document(docx_path)\n",
    "    return \"\\n\".join([para.text for para in doc.paragraphs])\n",
    "\n",
    "def extract_text_from_doc(doc_path):\n",
    "    word = client.Dispatch(\"Word.Application\")\n",
    "    doc = word.Documents.Open(doc_path)\n",
    "    text = doc.Content.Text\n",
    "    doc.Close()\n",
    "    word.Quit()\n",
    "    return text\n",
    "\n",
    "def extract_text_from_xml(xml_path):\n",
    "    tree = ET.parse(xml_path)\n",
    "    root = tree.getroot()\n",
    "    return \" \".join([elem.text for elem in root.iter() if elem.text])\n",
    "\n",
    "def extract_text_from_folder(folder_path):\n",
    "    extracted_texts = {}\n",
    "    for filename in os.listdir(folder_path):\n",
    "        file_path = os.path.join(folder_path, filename)\n",
    "        text = None\n",
    "\n",
    "        try:\n",
    "            if filename.lower().endswith(\".pdf\"):\n",
    "                text = extract_text_from_pdf(file_path)\n",
    "            elif filename.lower().endswith(\".docx\"):\n",
    "                text = extract_text_from_docx(file_path)\n",
    "            elif filename.lower().endswith(\".doc\"):\n",
    "                text = extract_text_from_doc(file_path)\n",
    "            elif filename.lower().endswith(\".xml\"):\n",
    "                text = extract_text_from_xml(file_path)\n",
    "            \n",
    "            if text:\n",
    "                extracted_texts[filename] = text\n",
    "        except Exception as e:\n",
    "            print(f\"Error extracting text from {filename}: {e}\")\n",
    "\n",
    "    return extracted_texts\n",
    "\n",
    "# Folder containing resumes\n",
    "folder_path = r\"C:\\Users\\Divya_prasath\\Downloads\\Profiles 1\"\n",
    "text_files = extract_text_from_folder(folder_path)\n",
    "\n",
    "# Initialize Gemini LLM model\n",
    "llm = ChatGoogleGenerativeAI(\n",
    "    model=\"gemini-1.5-flash\",\n",
    "    temperature=0,\n",
    "    max_tokens=None,\n",
    "    timeout=None,\n",
    "    max_retries=2,\n",
    "    api_key=\"AIzaSyC_1emKcfen2n5ZS-cjOTT37najYpBWmco\",\n",
    ")\n",
    "\n",
    "# Define data structure for extracted resume details\n",
    "class Resume(BaseModel):\n",
    "    name: str = Field(description=\"name from resume\")\n",
    "    phone: str = Field(description=\"phone number from resume\")\n",
    "    email: str = Field(description=\"email from resume\")\n",
    "    skill: str = Field(description=\"skill from resume\")\n",
    "\n",
    "parser = JsonOutputParser(pydantic_object=Resume)\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    template=\"Extract name, phone, email, skills from the given text resume.\\n{format_instructions}\\n{query}\\n\",\n",
    "    input_variables=[\"query\"],\n",
    "    partial_variables={\"format_instructions\": parser.get_format_instructions()},\n",
    ")\n",
    "\n",
    "chain = prompt | llm | parser\n",
    "\n",
    "details = []\n",
    "for filename, text in text_files.items():\n",
    "    retries = 3\n",
    "    success = False\n",
    "\n",
    "    while retries > 0 and not success:\n",
    "        try:\n",
    "            extracted_details = chain.invoke({\"query\": text})\n",
    "            details.append(extracted_details)\n",
    "            success = True\n",
    "            time.sleep(1)\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {filename}: {e}\")\n",
    "            retries -= 1\n",
    "            time.sleep(2 ** (3 - retries))\n",
    "\n",
    "# Convert extracted details into a DataFrame\n",
    "df = pd.DataFrame(details)\n",
    "\n",
    "# Define output folder and CSV file path\n",
    "output_folder = r\"C:\\Users\\Divya_prasath\\Desktop\\task\\extracted_data\"\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "csv_output_path = os.path.join(output_folder, \"resume.csv\")\n",
    "\n",
    "# Save extracted data to CSV file\n",
    "df.to_csv(csv_output_path, index=False)\n",
    "print(f\"Extracted resume details saved to: {csv_output_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Divya_prasath\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\elasticsearch\\_sync\\client\\__init__.py:403: SecurityWarning: Connecting to 'https://localhost:9200' using TLS with verify_certs=False is insecure\n",
      "  _transport = transport_class(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully indexed 105 documents into Elasticsearch!\n"
     ]
    }
   ],
   "source": [
    "from elasticsearch import Elasticsearch, helpers\n",
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "\n",
    "ES_CONFIG = { \n",
    "    \"host\": \"https://localhost:9200\",\n",
    "    \"username\": \"elastic\",\n",
    "    \"password\": os.getenv(\"ES_PASSWORD\"),  \n",
    "    \"index\": \"task3\"\n",
    "}\n",
    "\n",
    "# Initialize Elasticsearch connection\n",
    "es = Elasticsearch(\n",
    "    ES_CONFIG[\"host\"],\n",
    "    basic_auth=(ES_CONFIG[\"username\"], ES_CONFIG[\"password\"]),\n",
    "    verify_certs=False  \n",
    ")\n",
    "\n",
    "csv_file_path = r\"C:\\Users\\Divya_prasath\\Desktop\\task\\extracted_data\\resume.csv\"\n",
    "\n",
    "def push_data_to_elasticsearch(csv_file, es_client, index_name):\n",
    "    if not os.path.exists(csv_file):\n",
    "        print(\"CSV file not found!\")\n",
    "        return\n",
    "    \n",
    "    df = pd.read_csv(csv_file)\n",
    "\n",
    "    df.replace({np.nan: None}, inplace=True)\n",
    "\n",
    "    records = df.to_dict(orient=\"records\")\n",
    "\n",
    "    actions = [\n",
    "        {\n",
    "            \"_index\": index_name,\n",
    "            \"_source\": record\n",
    "        }\n",
    "        for record in records\n",
    "    ]\n",
    "\n",
    "    try:\n",
    "        helpers.bulk(es_client, actions)\n",
    "        print(f\"Successfully indexed {len(records)} documents into Elasticsearch!\")\n",
    "    except helpers.BulkIndexError as e:\n",
    "        print(f\"Bulk indexing error: {e}\")\n",
    "        for error in e.errors:\n",
    "            print(error)  \n",
    "\n",
    "push_data_to_elasticsearch(csv_file_path, es, ES_CONFIG[\"index\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Divya_prasath\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\elasticsearch\\_sync\\client\\__init__.py:403: SecurityWarning: Connecting to 'https://localhost:9200' using TLS with verify_certs=False is insecure\n",
      "  _transport = transport_class(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inserted DhineshKumar_SDE_Resume.pdf into Elasticsearch\n",
      "Inserted hi.pdf into Elasticsearch\n",
      "Inserted Jaya Sanjay - Resume.pdf into Elasticsearch\n",
      "Inserted NILESH SRINIVASAN.pdf into Elasticsearch\n",
      "Data extraction and indexing completed successfully!\n"
     ]
    }
   ],
   "source": [
    "from elasticsearch import Elasticsearch, helpers\n",
    "import os\n",
    "import pandas as pd\n",
    "import time\n",
    "import numpy as np\n",
    "import pdfplumber\n",
    "import xml.etree.ElementTree as ET\n",
    "from docx import Document\n",
    "from win32com import client\n",
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "import urllib3\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "urllib3.disable_warnings()\n",
    "\n",
    "# Elasticsearch configuration\n",
    "ES_CONFIG = {\n",
    "    \"host\": \"https://localhost:9200\",\n",
    "    \"username\": \"elastic\",\n",
    "    \"password\": os.getenv(\"ES_PASSWORD\"),  # Load password from .env file\n",
    "    \"index\": \"task3\"\n",
    "}\n",
    "\n",
    "# Initialize Elasticsearch connection\n",
    "es = Elasticsearch(\n",
    "    ES_CONFIG[\"host\"],\n",
    "    basic_auth=(ES_CONFIG[\"username\"], ES_CONFIG[\"password\"]),\n",
    "    verify_certs=False\n",
    ")\n",
    "\n",
    "\n",
    "def extract_text_from_pdf(pdf_path):\n",
    "    with pdfplumber.open(pdf_path) as pdf:\n",
    "        return \"\\n\".join([page.extract_text() for page in pdf.pages if page.extract_text()])\n",
    "\n",
    "def extract_text_from_docx(docx_path):\n",
    "    doc = Document(docx_path)\n",
    "    return \"\\n\".join([para.text for para in doc.paragraphs if para.text.strip()])\n",
    "\n",
    "def extract_text_from_doc(doc_path):\n",
    "    try:\n",
    "        word = client.Dispatch(\"Word.Application\")\n",
    "        word.Visible = False\n",
    "        doc = word.Documents.Open(doc_path)\n",
    "        text = doc.Content.Text.strip()\n",
    "        doc.Close(False)\n",
    "        word.Quit()\n",
    "        return text\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing DOC file {doc_path}: {e}\")\n",
    "        return \"\"\n",
    "\n",
    "def extract_text_from_xml(xml_path):\n",
    "    try:\n",
    "        tree = ET.parse(xml_path)\n",
    "        root = tree.getroot()\n",
    "        return \" \".join([elem.text.strip() for elem in root.iter() if elem.text])\n",
    "    except Exception as e:\n",
    "        print(f\"Error parsing XML {xml_path}: {e}\")\n",
    "        return \"\"\n",
    "\n",
    "\n",
    "def extract_text_from_folder(folder_path):\n",
    "    extracted_texts = {}\n",
    "    for filename in os.listdir(folder_path):\n",
    "        file_path = os.path.join(folder_path, filename)\n",
    "        text = None\n",
    "\n",
    "        try:\n",
    "            if filename.lower().endswith(\".pdf\"):\n",
    "                text = extract_text_from_pdf(file_path)\n",
    "            elif filename.lower().endswith(\".docx\"):\n",
    "                text = extract_text_from_docx(file_path)\n",
    "            elif filename.lower().endswith(\".doc\"):\n",
    "                text = extract_text_from_doc(file_path)\n",
    "            elif filename.lower().endswith(\".xml\"):\n",
    "                text = extract_text_from_xml(file_path)\n",
    "\n",
    "            if text and text.strip():\n",
    "                extracted_texts[filename] = text\n",
    "            else:\n",
    "                print(f\"Skipping empty file: {filename}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error extracting text from {filename}: {e}\")\n",
    "\n",
    "    return extracted_texts\n",
    "\n",
    "\n",
    "folder_path = r\"C:\\Users\\Divya_prasath\\Desktop\\task\\index resumes\"\n",
    "text_files = extract_text_from_folder(folder_path)\n",
    "\n",
    "\n",
    "api_key = os.getenv(\"GOOGLE_API_KEY\")  \n",
    "if not api_key:\n",
    "    raise ValueError(\"GEMINI_API_KEY is not set. Please set it as an environment variable.\")\n",
    "\n",
    "llm = ChatGoogleGenerativeAI(\n",
    "    model=\"gemini-1.5-flash\",\n",
    "    temperature=0,\n",
    "    max_tokens=None,\n",
    "    timeout=None,\n",
    "    max_retries=2,\n",
    "    api_key=api_key,\n",
    ")\n",
    "\n",
    "\n",
    "class Resume(BaseModel):\n",
    "    name: str = Field(description=\"name from resume\")\n",
    "    phone: str = Field(description=\"phone number from resume\")\n",
    "    email: str = Field(description=\"email from resume\")\n",
    "    skill: str = Field(description=\"skill from resume\")\n",
    "\n",
    "parser = JsonOutputParser(pydantic_object=Resume)\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    template=\"Extract name, phone, email, skills from the given text resume.\\n{format_instructions}\\n{query}\\n\",\n",
    "    input_variables=[\"query\"],\n",
    "    partial_variables={\"format_instructions\": parser.get_format_instructions()},\n",
    ")\n",
    "\n",
    "chain = prompt | llm | parser\n",
    "\n",
    "for filename, text in text_files.items():\n",
    "    retries = 3\n",
    "    success = False\n",
    "\n",
    "    while retries > 0 and not success:\n",
    "        try:\n",
    "            extracted_data = chain.invoke({\"query\": text})\n",
    "            extracted_details = Resume(**extracted_data) \n",
    "            doc = extracted_details.dict()  \n",
    "\n",
    "            es.index(index=ES_CONFIG[\"index\"], document=doc)  \n",
    "            print(f\"Inserted {filename} into Elasticsearch\")\n",
    "\n",
    "            success = True\n",
    "            time.sleep(1)\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {filename}: {e}\")\n",
    "            retries -= 1\n",
    "            time.sleep(2 ** (3 - retries))  \n",
    "\n",
    "print(\"Data extraction and indexing completed successfully!\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
